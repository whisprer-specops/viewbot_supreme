import requests
import random
import re
from bs4 import BeautifulSoup
from urllib.parse import urlparse

class PaywallBuster:
    def __init__(self, proxy_pool=None, user_agents=None):
        self.proxy_pool = proxy_pool or []
        self.user_agents = user_agents or [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0 Safari/537.36"
        ]

    def _generate_headers(self):
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept-Language': 'en-US,en;q=0.9',
            'Referer': 'https://google.com'
        }

    def _sanitize_html(self, html):
        soup = BeautifulSoup(html, 'html.parser')
        for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'aside']):
            tag.decompose()
        text_blocks = [p.get_text() for p in soup.find_all(['p', 'h1', 'h2'])]
        cleaned = re.sub(r'\\n+', '\\n\\n', '\\n'.join(text_blocks)).strip()
        return cleaned

    def _is_paywalled(self, html):
        paywall_patterns = [
            r'paywall', r'subscribe', r'content-protected', r'overlay', r'blocker'
        ]
        return any(re.search(pat, html, re.IGNORECASE) for pat in paywall_patterns)

    def extract_readable_text(self, url):
        proxy = {'http': None, 'https': None}
        if self.proxy_pool:
            chosen = random.choice(self.proxy_pool)
            proxy = {'http': f"http://{chosen}", 'https': f"http://{chosen}"}

        try:
            res = requests.get(url, headers=self._generate_headers(), proxies=proxy, timeout=10)
            html = res.text
            if self._is_paywalled(html):
                print("[!] Detected paywall. Returning sanitized content.")
            return self._sanitize_html(html)
        except Exception as e:
            print(f"[PaywallBuster] Failed to extract: {e}")
            return ""